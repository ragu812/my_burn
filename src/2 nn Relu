use std::usize;


use burn::nn::{Initializer, Linear, LinearConfig,Relu};
use burn::tensor::{Tensor,TensorData, Shape};
use burn::tensor::backend::Backend;
use burn::optim::{AdamConfig, GradientsParams, Optimizer};
use burn::module::Module;
// use burn_autodiff::Autodiff;

#[derive(Module,Debug)]
pub struct Pi<B: Backend>{
    linear1: Linear<B>,
    linear2: Linear<B>,
    activation: Relu,
}
impl<B:Backend> Pi<B>{
    pub fn new(device: &B::Device) -> Self {
        let linear1 = LinearConfig::new(1,10)
            .with_initializer(Initializer::KaimingUniform{ gain: (1.0), fan_out_only: false})
            .init(device);
        let linear2 = LinearConfig::new(10,1)
            .with_initializer(Initializer::KaimingUniform{gain: (1.0), fan_out_only: false})
            .init(device);
        let activation = Relu;
        Self{
            linear1,
            linear2,
            activation,
        }
    }
    pub fn forward(&self, input: Tensor<B,2>) ->Tensor<B,2>{
        let x = self.linear1.forward(input);
        let x = self.activation.forward(x);
        let x = self.linear2.forward(x);
        x
    }
}
fn main(){
    type Backend = burn_autodiff::Autodiff<burn::backend::NdArray>;
    let _device = <Backend as burn::prelude::Backend>::Device::default();
    // type AutodiffBackend=  Autodiff<Backend>;
    let device = <burn::backend::NdArray as burn::prelude::Backend>::Device::default();
   

    let mut model = Pi::new(&device);
     let mut optimizer = AdamConfig::new().init::<Backend, Pi<Backend>>();
    // to create a vector with odd numbers upto 2n-1
    let n: usize = 100;
    let mut input: Vec<usize> = Vec::new();
    for i in 0..n{
        input.push(2*i+1);
    }

    // converting the vector to a tensor of shape (n,1)

    let input_f32: Vec<f32> = input.iter().map(|&x| x as f32).collect();

    // creating a tensor of shape (n,1) from the input vector


    let denominator = Tensor::<Backend,2>::from_data(TensorData::new(input_f32, Shape::new([100,1])), &device);
    

    // creating a tensor of ones with shape (n,1)
    let ones = Tensor::<Backend,2>::ones([100,1],&device);

    // dividing ones by the denominator tensor
    let input1 = Tensor::<Backend,2>::div(ones,denominator);

    // To create alternate 1 and -1 signs
    let mut signs = Vec::new();
    let m: usize = n;
    for i in 0..m {
        if i % 2 == 0 {
            signs.push(1.0);
        } else {
            signs.push(-1.0);
        }
    }

    // convert signs Vec<f32> to a tensor
   let signs_tensor = Tensor::<Backend,2>::from_data(
        TensorData::new(signs, Shape::new([100, 1])), 
        &device
    );

    // multipying sign tensor and input 1
    let multiply = Tensor::<Backend,2>::mul(signs_tensor, input1.clone());
    
    // definfing an output tensor

    let pi_approximation = multiply.sum().mul_scalar(4.0);


    // Use input1 directly since it's not used after this point
    // Reshape _output_scalar to a 2D tensor of shape [1, 1]

    let input_tensor: Tensor<Backend, 2> = pi_approximation.reshape([1, 1]);
    let output: Tensor<_, 2> = model.forward(input_tensor.clone());

    // calculate the original pi value
    
    let pi_value = Tensor::<Backend,2>::from_data(
        TensorData::new(vec![std::f32::consts::PI], Shape::new([1, 1])),
        &device
    );

    let _pi_approximation = model.forward(output.clone()).reshape([1,1]).detach();
    let target_tensor = Tensor::<Backend,2>::from_floats([[std::f32::consts::PI]], &device);

    let epochs = 100;
    for epoch in 0..epochs {

        let output = model.forward(input_tensor.clone());
        let target_tensor = Tensor::<Backend,2>::from_floats([[std::f32::consts::PI]], &device);
        let loss = burn::nn::loss::MseLoss::new().forward(output.clone(),target_tensor.clone(), burn::nn::loss::Reduction::Mean);

        let grad = loss.backward();
        let grad_params = GradientsParams::from_grads(grad, &model);
        model = optimizer.step(1e-4, model, grad_params);

        if epoch % 1 == 0 {
            println!("Epoch: {:?} , Loss: {:?}", epoch,loss);
        }
    }
    let final_output = model.forward(input_tensor.clone());
    let final_loss = burn::nn::loss::MseLoss::new()
        .forward(final_output.clone(), target_tensor.clone(),
         burn::nn::loss::Reduction::Mean);


    // printing the loss value
    println!("\nFinal Loss: {:?}",final_loss.clone());

    // printing the output tensor
    println!("\nModel Value: {:?}",output.clone());

    println!("\n Pi: {:?}", pi_value);

}
